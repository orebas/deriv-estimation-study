\section{Methodology}
\label{sec:methodology}

\subsection{Test System}
\label{sec:test_system}

We selected the \textbf{Lotka-Volterra predator-prey model} as our test system, a canonical benchmark in dynamical systems.

\subsubsection{System Equations}

\begin{align}
\frac{dx}{dt} &= \alpha x - \beta xy \quad \text{(prey dynamics)} \\
\frac{dy}{dt} &= \delta xy - \gamma y \quad \text{(predator dynamics)}
\end{align}

where $x(t)$ is the prey population and $y(t)$ is the predator population.

\textbf{Parameters:}
\begin{itemize}
    \item $\alpha = 1.5$ (prey growth rate)
    \item $\beta = 1.0$ (predation rate)
    \item $\gamma = 3.0$ (predator death rate)
    \item $\delta = 1.0$ (predator growth from predation)
\end{itemize}

\textbf{Initial conditions:}
\begin{itemize}
    \item $x_0 = 1.0$ (prey)
    \item $y_0 = 1.0$ (predator)
\end{itemize}

\textbf{Time span:} $[0, 10]$ with 101 equally-spaced points ($\Delta t \approx 0.1$)

\textbf{Observable:} We tested derivative estimation on the \textbf{prey population $x(t)$}, which exhibits oscillatory behavior.

\textbf{Important limitation:} Only one variable (prey) from the two-state system was analyzed. Conclusions may not generalize even within the same system, as prey and predator trajectories have different oscillatory properties and noise sensitivities. Ideally, both variables should be evaluated.

\subsubsection{Ground Truth Generation}

High-precision ground truth was generated using the following rigorous procedure:

\begin{enumerate}
    \item \textbf{Symbolic differentiation:} Derivatives up to order 7 were computed symbolically using ModelingToolkit.jl's symbolic differentiation engine, yielding exact differential equations for each derivative order
    \item \textbf{Augmented system:} The original Lotka-Volterra equations were augmented with these symbolic derivative equations to create an extended ODE system containing $x(t), y(t), dx/dt, d^2x/dt^2, \ldots, d^7x/dt^7$ as state variables
    \item \textbf{Numerical integration:} The augmented system was solved using the Vern9 algorithm (9th-order Verner method) with absolute tolerance $10^{-12}$ and relative tolerance $10^{-12}$
    \item \textbf{Validation:} Analytical derivative formulas for orders $\geq 3$ in coupled nonlinear systems are intractable. Convergence was verified by comparing Vern9 solutions at tolerances $10^{-12}$ vs $10^{-14}$ for orders 0--5, showing agreement to $<10^{-10}$ relative error. Higher-order derivatives (6--7) are subject to greater numerical uncertainty and should be interpreted cautiously.
\end{enumerate}

This approach avoids interpolant-based automatic differentiation and provides high-accuracy numerical ground truth constrained by ODE solver precision (validated to $\sim 10^{-10}$ for orders 0--5).

\textbf{Sampling resolution concern:} With $\Delta t \approx 0.1$, high-order derivatives (especially orders 6--7) approach the Nyquist limit for oscillatory signals. This coarse resolution may suppress some methods due to aliasing rather than algorithmic weakness. Future work should include convergence testing with finer grids.

\textbf{Rationale for system choice:} Lotka-Volterra provides:
\begin{itemize}
    \item Nonlinear oscillatory dynamics representative of many scientific domains
    \item Symbolic differentiability enabling rigorous ground truth
    \item Moderate computational cost for extensive benchmarking
\end{itemize}

\subsection{Noise Model}
\label{sec:noise_model}

\textbf{Type:} Additive white Gaussian noise

\textbf{Scaling:} For each noise level $\varepsilon$, we added noise scaled by the clean signal's standard deviation:
\begin{equation}
y_{\text{noisy}}(t) = y_{\text{true}}(t) + \varepsilon \cdot \text{std}(y_{\text{true}}) \cdot \eta(t)
\end{equation}
where $\eta(t) \sim \mathcal{N}(0, 1)$ is standard normal noise.

\textbf{Rationale:} Constant-variance Gaussian noise is a standard model for measurement error. Scaling by signal standard deviation ensures consistent interpretation across different signals.

\textbf{Important limitation:} Additive noise can produce negative population values, violating biological constraints. Multiplicative noise (proportional to signal magnitude) would be more realistic but was not tested. Results may not generalize to measurement models where noise is strictly positive or heteroscedastic.

\textbf{Noise levels tested:} Seven levels spanning approximately 7 orders of magnitude:
\begin{itemize}
    \item $10^{-8}$ (near-noiseless)
    \item $10^{-6}$
    \item $10^{-4}$
    \item $10^{-3}$
    \item $10^{-2}$ (1\%)
    \item $2 \times 10^{-2}$ (2\%)
    \item $5 \times 10^{-2}$ (5\%)
\end{itemize}

\textbf{Interpretation:} For the Lotka-Volterra prey population with $\text{std}(x) \approx 0.29$, noise level $10^{-2}$ corresponds to absolute noise std $\approx 0.0029$.

\textbf{Randomization:} Three independent noise realizations per configuration using Mersenne Twister PRNG with seeds 12345, 12346, 12347 for trials 1--3 respectively, ensuring exact reproducibility.

\textbf{Pseudo-replication caveat:} All three trials use the same underlying trajectory; variability reflects only the noise model, not dynamical diversity. A more robust design would test multiple initial conditions or parameter sets.

\subsection{Experimental Design}
\label{sec:experimental_design}

\textbf{Configurations tested:}
\begin{itemize}
    \item 8 derivative orders (0--7)
    \item 7 noise levels ($10^{-8}$ to $5 \times 10^{-2}$)
    \item 3 trials per configuration
    \item \textbf{Total:} $8 \times 7 \times 3 = 168$ test cases per method
\end{itemize}

\textbf{Method coverage:}
\begin{itemize}
    \item \textbf{Full coverage} (all 56 order$\times$noise combinations): 16 methods
    \item \textbf{Partial coverage}: 11 methods
    \begin{itemize}
        \item Central-FD, TVRegDiff-Julia: 14/56 configurations (orders 0--1 only --- library provides stencils/regularization only up to 1st order)
        \item Dierckx-5, ButterworthSpline\_Python, Butterworth\_Python, Whittaker\_m2\_Python, fourier, fourier\_continuation, RKHS\_Spline\_m2\_Python, KalmanGrad\_Python, SVR\_Python: 42/56 configurations (orders 0--5, missing 6--7 --- degree/capability limits)
    \end{itemize}
\end{itemize}

\textbf{Endpoint treatment:} First and last evaluation points excluded from all error computations to avoid boundary effects, leaving 99 interior points for analysis.

\textbf{Important note:} Excluding edge points removes regions where many practical estimators exhibit boundary degradation. Metrics therefore evaluate performance on an idealized interior sub-problem and may overstate accuracy for applications requiring full-domain estimates.

\textbf{Failure handling:}
\begin{itemize}
    \item Methods returning NaN or Inf for $>80\%$ of evaluation points marked as failed for that configuration
    \item Failed configurations excluded from that method's statistics
    \item Partial failures documented separately
\end{itemize}

\textbf{Data pipeline:}
\begin{enumerate}
    \item Generate ground truth for Lotka-Volterra system once
    \item For each configuration (noise level $\times$ trial):
    \begin{itemize}
        \item Add noise to ground truth prey trajectory
        \item Export to JSON for Python methods
        \item Evaluate all Julia methods in-process
        \item Call Python script with 300s timeout
        \item Collect results (predictions, timing, success status)
    \end{itemize}
    \item Aggregate results across trials
\end{enumerate}

\subsection{Evaluation Metrics}
\label{sec:metrics}

\subsubsection{Root Mean Squared Error (RMSE)}

\textbf{Definition:}
\begin{equation}
\text{RMSE} = \sqrt{\text{mean}((y_{\text{pred}} - y_{\text{true}})^2)}
\end{equation}

Computed over interior points only (indices 2 to $n-1$).

\textbf{Limitation:} RMSE values are not comparable across derivative orders because derivative magnitudes vary by orders of magnitude (e.g., $\text{std}(x) \approx 0.3$ vs $\text{std}(d^7x/dt^7) \approx 10^{-4}$).

\subsubsection{Mean Absolute Error (MAE)}

\textbf{Definition:}
\begin{equation}
\text{MAE} = \text{mean}(|y_{\text{pred}} - y_{\text{true}}|)
\end{equation}

\textbf{Advantage:} Robust to outliers

\textbf{Limitation:} Same cross-order comparison issue as RMSE

\subsubsection{Normalized RMSE (nRMSE) --- Primary Metric}

\textbf{Definition:}
\begin{equation}
\text{nRMSE} = \frac{\text{RMSE}}{\text{std}(y_{\text{true}})}
\end{equation}

\textbf{Critical clarification:} The standard deviation in the denominator is computed \textbf{for the specific derivative order being evaluated}, not the base signal. Thus:
\begin{itemize}
    \item For order 0: nRMSE = RMSE$(x)$ / std$(x)$
    \item For order 1: nRMSE = RMSE$(dx/dt)$ / std$(dx/dt)$
    \item For order 7: nRMSE = RMSE$(d^7x/dt^7)$ / std$(d^7x/dt^7)$
\end{itemize}

This makes nRMSE order-comparable: a value of 0.2 means ``error is 20\% of typical variation in that specific derivative'' regardless of order.

\textbf{Interpretation:} Error expressed as a fraction of signal variation

\textbf{Performance thresholds} (calibrated via visual inspection of derivative plots):
\begin{itemize}
    \item $< 0.1$: Visually accurate reconstruction
    \item $0.1$--$0.3$: Moderate deviation visible
    \item $0.3$--$1.0$: Substantial error but structure recognizable
    \item $> 1.0$: Error exceeds typical signal variation
\end{itemize}

\textbf{Note:} These thresholds are empirical guidelines, not rigorously validated cutoffs. Readers should interpret absolute nRMSE values directly.

\textbf{Justification:} Absolute metrics (RMSE, MAE) favor low-order derivatives where magnitudes are smaller. Normalized metrics enable fair comparison across orders, revealing which methods handle noise amplification effectively.

\textbf{Zero-crossing robustness:} Normalization uses std$(y_{\text{true}})$ computed over the same 99 interior evaluation points as RMSE (not including endpoints), avoiding division-by-zero when derivatives cross zero while maintaining consistency between numerator and denominator domains.

\subsection{Statistical Analysis and Limitations}
\label{sec:statistics}

\textbf{Central tendency:} Mean nRMSE across 3 trials reported as primary statistic

\textbf{Uncertainty quantification:}
\begin{itemize}
    \item Standard deviation across 3 trials
    \item 95\% confidence intervals computed using $t$-distribution with df=2
\end{itemize}

\textbf{CRITICAL LIMITATION --- Insufficient Statistical Power:}

With only $n=3$ trials per configuration:
\begin{itemize}
    \item 95\% CI half-width = $t_{0.975,2} \times \text{SD} / \sqrt{3} \approx 2.48 \times \text{SD}$ (extremely wide with df=2)
    \item \textbf{No formal hypothesis tests performed} due to insufficient sample size
    \item Claims of method superiority are \textbf{exploratory}, not statistically definitive
    \item CI overlap/non-overlap provides \textbf{suggestive evidence only}, not formal significance
\end{itemize}

\textbf{Interpretation guidelines:}
\begin{itemize}
    \item Non-overlapping CIs across methods $\rightarrow$ \textbf{moderate evidence} of difference (not ``strong'' or ``statistically significant'')
    \item Overlapping CIs $\rightarrow$ \textbf{insufficient evidence} to claim difference (NOT ``no difference'')
    \item Consistent ordering across all 3 trials $\rightarrow$ \textbf{robust ranking pattern}
\end{itemize}

\textbf{Rationale for $n=3$:} Computational cost (24 methods $\times$ 168 configs $\times$ median 0.5s = $\sim$5 hours total) was balanced against basic repeatability verification. This is sufficient to detect catastrophic failures and gross performance differences, but \textbf{inadequate for fine-grained method ranking} or quantifying small effect sizes.

\textbf{Pseudo-replication:} Trials differ only in random noise seed, not in underlying dynamics. Variability reflects noise model, not biological or parameter diversity.

\textbf{Recommended interpretation:} Treat rankings as \textbf{descriptive summaries} of performance on this specific system, not as statistically validated general statements. Methods that differ by $<2\times$ in nRMSE should be considered comparable given sample size.

\textbf{Future work:} Testing on 10+ diverse signals (varied ICs, parameters, systems) with $\geq 10$ trials each is needed for robust statistical inference (see Section~\ref{sec:future_work}).

\subsection{Hyperparameter Optimization Protocol}
\label{sec:hyperparameters}

\textbf{Objective:} Provide equivalent optimization effort to all tunable methods.

\textbf{Gaussian Processes:}
\begin{itemize}
    \item Hyperparameters: length scale ($\ell$), signal variance ($\sigma^2_f$), noise variance ($\sigma^2_n$)
    \item Optimization: Maximum Likelihood Estimation (MLE) via L-BFGS-B
    \item Implementation: GaussianProcesses.jl (Julia), scikit-learn (Python)
    \item Bounds: $\ell \in [0.01, 10]$, $\sigma^2 \in [0.01, 10]$ (scaled to problem's time domain and signal variance)
    \item Initialization: 3 random restarts (seeded deterministically per trial: seed + restart\_idx) to mitigate local minima while ensuring reproducibility
\end{itemize}

\textbf{Splines:}
\begin{itemize}
    \item Smoothing parameter ($\lambda$) selected via Generalized Cross-Validation (GCV)
    \item Automatic per-dataset tuning
    \item Implementation: Dierckx.jl (Julia), scipy.interpolate (Python)
\end{itemize}

\textbf{AAA Rational Approximation:}
\begin{itemize}
    \item Tolerance: $10^{-13}$ (greedy termination criterion)
    \item Fixed for all evaluations (non-tunable)
    \item Precision: BigFloat (256-bit) for AAA-HighPrec, Float64 for AAA-LowPrec
\end{itemize}

\textbf{Fourier Spectral:}
\begin{itemize}
    \item Filter fraction: 0.4 (retains lower 40\% of frequency spectrum)
    \item \textbf{Optimization procedure:} Preliminary grid search over [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95] on a \textbf{separate validation run} (Lotka-Volterra with noise=$10^{-3}$, orders 0--7), selecting value minimizing mean rank
    \item Fixed at 0.4 for all reported test results
\end{itemize}

\textbf{Methodological concern:} Unlike GP (per-dataset MLE) and splines (per-dataset GCV), the Fourier filter fraction was pre-tuned and fixed. This may provide an advantage relative to methods without access to validation data. However, 0.4 is a reasonable default for oscillatory signals and represents typical practitioner usage. Alternative: implement per-dataset tuning via cross-validation (adds computational cost).

\textbf{Total Variation (TVRegDiff):}
\begin{itemize}
    \item Regularization parameter ($\alpha$) auto-tuned via internal algorithm
    \item Iteration limit: 100, convergence tolerance: $10^{-6}$
\end{itemize}

\textbf{Finite Differences:}
\begin{itemize}
    \item No tunable parameters (stencil determined by order and point count)
\end{itemize}

\textbf{Computational budget:} 300 second timeout per evaluation

\subsection{Experimental Controls and Data Integrity}
\label{sec:controls}

\subsubsection{Exclusion Criteria}

Methods were excluded from final analysis if they met any condition below. \textbf{Important caveat:} The specific criteria were established during initial data exploration, not pre-registered before data collection. This creates potential for apparent cherry-picking.

\textbf{Criteria:}

\begin{enumerate}
    \item \textbf{Cross-language implementation failure:} When the same algorithm has implementations in both Julia and Python, and one shows $>50\times$ worse mean nRMSE after parameter parity verification
    \item \textbf{Numerical breakdown:} Mean nRMSE $> 10^6$ across all configurations
    \item \textbf{Coverage failure:} Method fails (NaN/Inf) on $>80\%$ of test configurations
\end{enumerate}

\textbf{Note on criterion 1:} The $50\times$ threshold is pragmatic but arbitrary. Language ecosystems naturally differ; ideally both implementations should be debugged to parity. We document both results where feasible (see Section~\ref{sec:exclusions}).

\subsubsection{Methods Excluded}
\label{sec:exclusions}

\textbf{Full candidate list:} 27 methods initially evaluated

\textbf{Excluded (3 methods):}

\textbf{1. GP-Julia-SE} (Gaussian Process with Squared Exponential kernel):
\begin{itemize}
    \item Mean nRMSE: 38,238,701 (catastrophic numerical failure)
    \item Likely cause: Hyperparameter optimization collapse (length scale $\rightarrow 0$ or $\rightarrow \infty$) or kernel derivative implementation error
    \item Decision: Exclude; functional GP alternatives exist (GP-Julia-AD, GP\_RBF\_Python)
    \item \textbf{Transparency note:} Implementation may be debuggable; exclusion based on observed failure, not theoretical limitation
\end{itemize}

\textbf{2. TVRegDiff\_Python} (Total Variation Regularized Differentiation):
\begin{itemize}
    \item Mean nRMSE: 14.186 ($72\times$ worse than Julia implementation: 0.195)
    \item \textbf{Parameter parity checks performed:}
    \begin{itemize}
        \item Regularization $\alpha$ matched across languages
        \item Boundary conditions verified (periodic vs natural)
        \item Iteration limits synchronized (100 max)
        \item Convergence tolerances matched ($10^{-6}$)
    \end{itemize}
    \item Decision: Exclude Python version; retain Julia version (TVRegDiff-Julia)
    \item \textbf{Transparency note:} Despite parity efforts, discrepancy persists; may reflect subtle algorithmic differences not captured by parameter matching
\end{itemize}

\textbf{3. SavitzkyGolay\_Python:}
\begin{itemize}
    \item Mean nRMSE: 15,443 ($17,500\times$ worse than Julia: 0.881)
    \item Likely cause: Despite attempts to match window size and polynomial degree parameters, performance remained drastically inferior, suggesting implementation differences beyond parameter tuning
    \item Decision: Exclude Python version; retain Julia version (Savitzky-Golay)
\end{itemize}

\textbf{Exclusion impact:} Final analysis includes 24 of 27 candidates. Exclusions documented as implementation quality findings (Section~\ref{sec:cross_language}).

\subsubsection{Coverage Accounting}

\textbf{Full coverage} (56/56 configs): 16 methods

\textbf{Partial coverage:} 11 methods (see list in Section~\ref{sec:experimental_design})

\textbf{Ranking policy:}
\begin{itemize}
    \item Overall rankings computed only over configurations where methods were tested
    \item Coverage percentages reported in all ranking tables
    \item \textbf{Naive rankings are misleading:} Methods tested only on easy configurations (low orders/noise) appear artificially superior
    \item \textbf{Fair comparison:} Primary rankings restricted to 16 full-coverage methods
\end{itemize}

\subsubsection{Runtime Measurement Standardization}

\textbf{Hardware:} \TODO{Fill exact CPU model from system info}
\begin{itemize}
    \item CPU: AMD/Intel \TODO{specific model}
    \item RAM: 32 GB
    \item OS: Linux 6.6.87.2 (WSL2)
\end{itemize}

\textbf{Precision consistency:}
\begin{itemize}
    \item Timing measured at same numerical precision as accuracy runs
    \item AAA-HighPrec: BigFloat for both
    \item All others: Float64
\end{itemize}

\textbf{Timing procedure:}
\begin{enumerate}
    \item Exclude data preprocessing (JSON I/O, array allocation)
    \item Time only: model fitting + derivative evaluation
    \item Julia methods: 1 warm-up run to exclude JIT compilation
    \item Report median across 3 trials
\end{enumerate}

\textbf{Timeout:} 300 seconds $\rightarrow$ method marked as failed

\subsubsection{Endpoint and Boundary Handling}

\textbf{Evaluation grid:} All methods evaluated on identical 99 interior points (indices 2:100)

\textbf{Boundary treatment (method-specific):}
\begin{itemize}
    \item \textbf{Fourier:} Symmetric extension for periodicity (note: Lotka-Volterra trajectories are oscillatory but not strictly periodic; this assumption may introduce edge artifacts despite interior-only evaluation)
    \item \textbf{Splines:} Natural boundary conditions ($d^2y/dx^2=0$ at endpoints)
    \item \textbf{Finite differences:} Stencils shrink at boundaries; edges excluded
    \item \textbf{GP:} No special treatment (kernel extrapolates naturally)
\end{itemize}

\textbf{Consistency check:} Verified all methods produce predictions on same grid before error computation

\subsubsection{Statistical Validity}

Covered in Section~\ref{sec:statistics} (merged to eliminate redundancy).

\subsection{Software and Implementation}
\label{sec:software}

\textbf{Julia environment:}
\begin{itemize}
    \item Version: 1.9.3
    \item Key packages:
    \begin{itemize}
        \item DifferentialEquations.jl 7.9.1
        \item GaussianProcesses.jl 0.12.5
        \item FFTW.jl 7.7.1
        \item Dierckx.jl 0.5.3
        \item ModelingToolkit.jl, Symbolics.jl (ground truth)
    \end{itemize}
\end{itemize}

\textbf{Python environment:}
\begin{itemize}
    \item Version: 3.11.5
    \item Key packages: numpy 1.25.2, scipy 1.11.2, scikit-learn 1.3.1
\end{itemize}

\textbf{Code availability:}
\begin{itemize}
    \item Repository: \TODO{Add GitHub URL and commit hash}
    \item Zenodo archive: \TODO{Add DOI upon publication}
    \item License: MIT
    \item Includes: All source code, configuration files, raw results CSV, figure generation scripts
\end{itemize}

\textbf{Reproducibility provisions:}
\begin{itemize}
    \item Fixed random seeds ensure exact noise realization reproducibility
    \item Environment specifications: \texttt{Project.toml} (Julia), \texttt{requirements.txt} (Python)
    \item Docker container: \TODO{Decide if providing, else remove line}
\end{itemize}

\textbf{Full workflow:} \texttt{src/comprehensive\_study.jl} orchestrates all steps; see README for invocation.
