\section{Method Catalog and Implementation}
\label{sec:methods}

This study did not begin with a fixed set of methods. Rather, it followed an exploratory research path, starting with methods known to work on noiseless data and progressively incorporating more sophisticated techniques in response to the challenges posed by noisy signals. This section provides a narrative of that journey, cataloging the methods investigated and detailing the significant implementation work required to create a fair and robust comparison.

\subsection{A Composable Framework: Approximation + Automatic Differentiation}
\label{sec:composable_framework}

A core finding of our study is the power of a composable, two-step framework for numerical differentiation. Rather than relying on bespoke algorithms for each derivative order, the most successful methods, particularly Gaussian Process Regression, implicitly or explicitly follow a simple and highly effective recipe:

\begin{enumerate}
    \item \textbf{Fit an Approximant:} First, a smooth, analytic function (the "approximant") is fitted to the noisy data. This function can be a Gaussian Process, a spline, a spectral representation, or any other differentiable model. The goal of this step is to capture the underlying signal while filtering out noise.
    \item \textbf{Differentiate via AD:} Second, this fitted function is differentiated using Automatic Differentiation (AD). Because the approximant is a well-defined mathematical function, AD can compute its derivatives to machine precision, free from the truncation and differencing errors that plague finite difference methods.
\end{enumerate}

This "Approximant-AD" pattern is exceptionally powerful because it decouples the problem of noise-robust smoothing from the problem of differentiation. It allows practitioners to leverage the vast and mature ecosystems of both statistical modeling and automatic differentiation, effectively bringing the full power of modern AD frameworks to bear on a classical numerical problem.

\textbf{The Critical Role of Taylor-Mode AD for High Orders:} While the Approximant-AD framework provides an elegant solution, the choice of AD implementation becomes paramount when targeting high-order derivatives (orders 5, 6, and 7). Naively composing first-order AD operations—either forward or reverse mode—to compute a $k$-th derivative results in exponential computational complexity, rendering it infeasible for orders beyond a handful.

To overcome this barrier, our framework employs \textbf{Taylor-mode automatic differentiation}. Instead of propagating only first-order derivative information, Taylor-mode AD propagates a full Taylor series expansion through the computation. By computing and carrying forward the Taylor coefficients at each step, it can calculate all derivatives up to order $k$ in a single pass with polynomial complexity. This efficiency gain is not merely an optimization but an enabling technology: the success of our top-performing methods, particularly \texttt{GP-Julia-AD}, is critically dependent on their use of Taylor-mode AD. As demonstrated in our results, the choice of AD implementation is as fundamental to success as the choice of the approximant itself.

\subsection{Breadth of the Study}
\label{sec:breadth}

For practitioners, the choice of a specific software package can be as consequential as the choice of the underlying algorithm. To this end, our study intentionally includes multiple implementations of similar or identical algorithms (e.g., several variants of Gaussian Process Regression and Fourier-based methods) to assess whether real-world performance differences arise from implementation details. While we could not be exhaustive, we sought to include representatives from all major methodological families as well as several novel approaches.

\subsection{Method Selection and Refinement}
\label{sec:method_selection}

The selection of numerical differentiation methods for this benchmark was an iterative process, beginning with a broad survey of established techniques and culminating in a rigorously filtered cohort of 31 contenders. This process can be understood as a sequence of three overlapping phases of exploration, expansion, and systematic refinement.

\begin{enumerate}
    \item \textbf{Initial Foray with Established Methods:} Our investigation began with methods well-established in noise-free or low-noise contexts. This initial pool included classical finite difference schemes, smoothing splines, and the Savitzky-Golay filter, which combines local polynomial regression with differentiation. We also evaluated the Adaptive Antoulas-Anderson (AAA) algorithm, a powerful tool for rational approximation in noise-free settings. However, during preliminary testing on noisy signals, the AAA-based differentiation methods demonstrated significant instability. The resulting derivative estimates frequently contained errors several orders of magnitude larger than those from other methods, rendering them unsuitable for this benchmark's focus on noisy time-series data. Consequently, while acknowledging its utility in other domains, the AAA method was the first to be excluded from our candidate pool.

    \item \textbf{Expansion to Noise-Robust Approaches:} Recognizing the limitations of methods not explicitly designed for noise, the second phase expanded our search to include techniques with inherent denoising or statistical modeling capabilities. This broadened the candidate pool to include methods based on wavelet filtering, Gaussian Process Regression (GPR), Total Variation Regularization, and Kalman filtering. These approaches are theoretically better suited to the challenges of differentiating noisy data, and their inclusion ensured the benchmark would cover a diverse range of modern, noise-robust strategies. This phase concluded with a comprehensive, albeit heterogeneous, collection of potential methods for systematic evaluation.

    \item \textbf{Systematic Filtering and Final Cohort Definition:} The final phase involved a rigorous, systematic filtering of all collected candidates to establish a final, consistent, and comparable set for analysis. Each method was evaluated against two primary criteria: stability (as discussed for AAA) and completeness of coverage across the required derivative orders (0 through 7) and test systems. Several promising algorithms were excluded at this stage because they either failed to compute derivatives for the entire range of orders or produced failures on specific dynamical systems, leaving gaps in the results matrix.

    This stringent filtering process yielded the final cohort of 31 methods that provided complete data coverage. From this group, we define the primary \textit{contender set} as those methods that successfully produced stable estimates for all systems up to the 5th derivative, forming the basis for the core analysis in this paper. The cohort includes implementations in both Python and Julia, spanning multiple algorithmic families: Gaussian Process methods, spline interpolation, spectral approaches, filtering techniques, and Total Variation regularization. This multi-stage refinement ensures that our benchmark compares only the most robust and broadly applicable differentiation techniques.
\end{enumerate}

\subsection{Implementation: The Challenge of High-Order Derivatives}
\label{sec:implementation_challenges}

A significant practical challenge was that almost no off-the-shelf package natively supports the computation of arbitrarily high-order derivatives. Overcoming this required substantial implementation effort.

\begin{itemize}
    \item \textbf{Augmentation with Automatic Differentiation:} For methods whose underlying approximant was differentiable (e.g., Gaussian Processes), we augmented the implementation with Automatic Differentiation (AD) to compute the derivatives. Where possible, we used Taylor-mode AD, which is essential for the efficient computation of high-order derivatives, as naive nested AD has exponential complexity.
    \item \textbf{Analytic Derivatives:} For other methods, such as those based on splines or Fourier series, we derived and implemented the analytical expressions for their higher-order derivatives directly.
    \item \textbf{Noise Model Integration:} In some cases, we also had to implement custom noise-estimation steps to provide the algorithms with required hyperparameters, using techniques such as wavelet MAD or simple finite-difference-based noise estimation.
\end{itemize}

This substantial, and often non-trivial, implementation work was a prerequisite for creating the level playing field upon which this benchmark is built.

% Method catalog removed - 31 tested methods listed in results
