\section{Methodology and Implementation}
\label{sec:methods}

The derivative estimation methods evaluated in this study were selected through a systematic survey and filtering process designed to cover a broad range of algorithmic approaches. The foundational background for the method families considered in this work—including local polynomial filters, splines, global basis functions, regularization, and probabilistic methods—is detailed in Section~\ref{sec:foundational_methodologies}.

This section details the specific criteria for method selection, key implementation choices for computing high-order derivatives, and the precise implementation of our top-performing Gaussian Process Regression approach.

\subsection{Systematic Filtering and Final Selection}
\label{sec:filtering_criteria}

From the initial survey, a rigorous multi-stage filtering process was applied to identify methods suitable for benchmarking:

\textbf{Stage 1: Stability Assessment.} Methods were first evaluated for numerical stability on noisy data. For example, the Adaptive Antoulas-Anderson (AAA) algorithm, while exceptionally powerful for rational approximation in noise-free contexts~\cite{Nakatsukasa_etal_2018, Bassik_etal_2023} (and our preferred interpolation method in that context), demonstrated significant instability when applied to noisy signals. The resulting derivative estimates frequently contained errors several orders of magnitude larger than other methods. The least stable methods were excluded from further analysis.


\textbf{Stage 2: Coverage Requirement.} We initially planned to require packages to support arbitrary derivative orders. In practice, orders $6$–$7$ (and often $5$) are both rarer in applications and substantially more fragile. Accordingly, our \emph{primary ranking} requires methods to produce valid derivatives for orders $0$–$5$ (across all systems and noise levels). Methods with narrower scope—whether due to theoretical limits (e.g., low-degree local polynomials) or implementation constraints—are documented but omitted from the primary comparison. Orders $6$–$7$ are reported separately as a stress test (Appendix~X). This criterion preserves a like-for-like comparison among methods with comparable capabilities.

\textbf{Stage 3: Implementation Verification.} Each remaining method was verified to produce valid output across all test configurations (three dynamical systems, seven noise levels). Methods that failed consistently on specific systems or noise regimes were excluded.

This systematic filtering yielded a final cohort of 28 methods that demonstrated both numerical stability and complete coverage for orders 0--5, forming the basis for the comparative analysis presented in Section~\ref{sec:results}. Methods are implemented in both Python and Julia, spanning the algorithmic families described above: Gaussian Process methods, spline interpolation, spectral approaches, filtering techniques, and Total Variation regularization.

\subsection{Implementation Considerations for High-Order Derivatives}
\label{sec:implementation_challenges}

A significant practical challenge in benchmarking derivative estimation methods is the computation of arbitrarily high-order derivatives, as most existing packages support only first or second derivatives. Three complementary approaches were employed in this study to address this limitation:

\begin{itemize}
    \item \textbf{Augmentation with Automatic Differentiation:} For methods whose underlying approximant was differentiable (e.g., Gaussian Processes), implementations were augmented with AD capabilities. Our Julia implementation leverages Taylor-mode AD~\cite{TaylorDiff_jl} for efficient computation of high-order derivatives.
    
    \item \textbf{Analytic Derivative Computation:} For methods such as splines or Fourier series, analytic expressions for higher-order derivatives were derived and implemented directly. This approach provides exact derivatives of the approximant function while maintaining computational efficiency.
    
    \item \textbf{Noise Estimation Integration:} Several methods require noise level estimates as hyperparameters. Where not provided by the original implementation, noise estimation routines based on wavelet MAD (Median Absolute Deviation)~\cite{Donoho_Johnstone_1994} or simple finite-difference variance estimation~\cite{Gasser_etal_1986} were integrated.
\end{itemize}

These implementation enhancements ensured a level playing field for comparison while maintaining the essential characteristics of each method.

\subsection{Gaussian Process Regression Implementation}
\label{sec:gpr_implementation}

Given the exceptional performance of Gaussian Process methods in our results, we provide specific implementation details for reproducibility. Our GPR approach~\cite{Fairbrother_etal_2022} follows a standard but carefully optimized workflow:

\begin{enumerate}
    \item \textbf{Kernel Selection:} We employ the squared exponential (RBF) kernel~\cite{Rasmussen_Williams_2006}: $k(x, x') = \sigma^2_f \exp(-\|x - x'\|^2 / 2\ell^2)$, where $\ell$ is the length scale and $\sigma_f$ is the signal variance.

    \item \textbf{Hyperparameter Optimization:} The hyperparameters $(\ell, \sigma_f, \sigma_n)$ are optimized via maximum likelihood estimation using L-BFGS-B optimization~\cite{Byrd_etal_1995} on the log marginal likelihood.

    \item \textbf{Derivative Computation:} Rather than using kernel derivatives, we extract the posterior mean function and differentiate it directly using automatic differentiation. This approach provides exact derivatives of the smooth interpolant.

    \item \textbf{Taylor-mode AD:} The Julia implementation leverages Taylor-mode automatic differentiation~\cite{TaylorDiff_jl} for efficient computation of derivatives up to order 7 in a single forward pass. Python implementations lack this capability, explaining their 10× slower performance.
\end{enumerate}

This ``fit-then-differentiate'' approach treats GPR as a sophisticated smoothing method, discarding the uncertainty quantification machinery after fitting to focus solely on the mean prediction and its derivatives.

% Method catalog removed - 31 tested methods are listed in results section and detailed in Appendix A