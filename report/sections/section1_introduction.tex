\section{Introduction}
\label{sec:introduction}

Derivative estimation from noisy data is a fundamental and notoriously ill-posed problem spanning computational science, engineering, and data analysis. Small noise perturbations in the input signal can produce arbitrarily large errors in derivative estimates, a challenge that intensifies rapidly with the derivative order. Despite this, reliable derivatives are essential in many settings, including:

\begin{itemize}
    \item \textbf{Dynamical Systems Identification:} Inferring governing equations from time-series or field data requires accurate derivative estimates to identify differential equations~\cite{Brunton_etal_2016}.
    \item \textbf{Signal Processing:} Edge detection, feature extraction, and change-point analysis all depend on robust derivative computation.
    \item \textbf{Control Systems:} Model-predictive and adaptive control rely on real-time derivative estimation for system state feedback.
    \item \textbf{Physics-Informed Machine Learning:} Enforcing physical conservation laws (PDE constraints) requires differentiating neural network outputs with respect to noisy inputs~\cite{Raissi_etal_2019}.
    \item \textbf{Scientific Data Analysis:} Estimating rates of change from experimental measurements, such as reaction rates from concentration data or acceleration from position measurements.
\end{itemize}

Our investigation is motivated by the challenges of parameter estimation in ordinary differential equations (ODEs), where accurate derivative estimates are crucial. ODEs provide an ideal testbed for differentiation methods: the structure of an ODE system, $\dot{\mathbf{x}} = f(\mathbf{x})$, provides a natural source of high-precision ground-truth data. Higher-order derivatives can be generated by repeatedly differentiating the system's equations, e.g., $\ddot{\mathbf{x}} = \frac{\partial f}{\partial \mathbf{x}} \dot{\mathbf{x}}$. This process, performed symbolically or via system augmentation, allows for the creation of a validation dataset without resorting to a separate numerical differentiation method, thus avoiding circularity. Our previous work demonstrated the potential of certain methods, such as the AAA algorithm~\cite{Nakatsukasa_etal_2018} with automatic differentiation, for differentiating noiseless data from dynamical systems~\cite{Bassik_etal_2023}.

Despite the importance of this problem, systematic comparative evaluation of numerical differentiation methods remains largely missing from the literature~\cite{Listmann_etal_2013, Knowles_2009, Walker_1998}. Existing studies suffer from several limitations:
\begin{enumerate}
    \item \textbf{Limited Scope:} They typically evaluate only a handful of methods on low-order derivatives (first or second), neglecting high-order derivatives where performance diverges dramatically (e.g.,~\cite{Listmann_etal_2013, Knowles_2009}).
    \item \textbf{Single-Noise Regime:} They test either noiseless or high-noise cases, missing crucial performance transitions across noise levels.
    \item \textbf{Incomplete Coverage Transparency:} Methods that fail at high orders or under high noise are often excluded without clear documentation, obscuring practical limits.
\end{enumerate}

To address this gap, the present study provides a comprehensive, transparent, and reproducible benchmark of numerical differentiation methods. We employ a systematic elimination process, starting from a broad set of hypotheses and progressively refining our analysis to arrive at specific, actionable recommendations. Our evaluation is conducted across three distinct dynamical systems and a wide spectrum of noise levels, ensuring the robustness and generalizability of our findings. The primary contribution of this work is therefore not merely to rank methods, but to provide the reader with a deeper understanding of the principles that govern their success and failure in different scenarios, ultimately equipping them with clear guidance for their own data analysis challenges.

\textbf{The main contributions of this paper are:}
\begin{itemize}
    \item A comprehensive benchmark of 28 numerical differentiation methods tested across derivative orders 0-7 (primary analysis on orders 1-5), seven noise levels (from $10^{-8}$ to 2\%), and three dynamical systems
    \item Identification of the composable ``Approximant-AD'' framework as a key pattern for success, combining smooth function fitting with automatic differentiation
    \item Demonstration that Gaussian Process regression with Taylor-mode AD provides the most robust performance across all conditions
    \item A practical three-tier framework for method selection based on speed-accuracy trade-offs
    \item Complete reproducible code, data, and analysis pipeline to support future research and practitioner adoption
\end{itemize}
