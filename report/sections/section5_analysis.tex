\section{Detailed Analysis: From Raw Data to Final Rankings}
\label{sec:analysis}

The journey from raw experimental output to the final summary table involved a deliberate, multi-stage filtering and analysis process. This section details the methodology used to distill the results and arrive at our final conclusions.

\subsection{Initial Data Filtering: Removing Unviable Methods}
The first step was to filter out methods that were fundamentally unsuitable or unstable for the task. This was not based on performance, but on viability.

\begin{enumerate}
    \item \textbf{Exclusion of Unstable Methods (AAA)}: Our implementations of the AAA (Adaptive Antoulas-Anderson) algorithm were found to be unstable for this specific task. Their errors were often orders of magnitude larger than any other method, and they frequently failed to produce valid output.
    
    It is important to frame this result: Our conclusion is not that rational approximants are unsuitable in general, but rather that a direct application does not appear to be robust to the combination of noise and differentiation. The AAA algorithm is exceptionally powerful for function approximation in noise-free contexts. We suspect that more sophisticated approaches, perhaps involving regularization or hybrid methods (e.g., fitting a rational approximant to a pre-smoothed signal), could unlock their potential. This remains a promising avenue for future research.
    
    Given these results, the AAA methods were removed from the main analysis cohort to prevent their extreme outliers from skewing aggregate statistics.

    \item \textbf{Exclusion of Incomplete Coverage}: A primary goal of this study was to evaluate methods across a wide range of derivative orders (0 through 7). Several methods, particularly some of the Python legacy methods and those based on low-degree splines, did not have full coverage across all orders or failed consistently on certain systems. To ensure a fair, apples-to-apples comparison in our final rankings, only methods that successfully produced results for all tested configurations were included in the "contender" set.
\end{enumerate}

\subsection{Defining the "Contender" Set}
After this initial filtering, we were left with a set of robust methods that provided full data coverage (up to order 5 for our main summary). This cohort became our "contender" set for the final analysis. All subsequent rankings and comparisons were performed \textit{only within this group}. 
%This is a crucial methodological point: the ranks presented in the summary table are ranks \textit{among the contenders}, not among the initial, larger pool of all tested methods.

\subsection{Ranking Methodology}
To produce the final summary table, we followed a two-step ranking process:
\begin{enumerate}
    \item \textbf{Per-Cell Ranking}: For each individual experimental cell—defined by a unique combination of \texttt{(ODE\_system, noise\_level, derivative\_order)}—we ranked the contender methods against each other based on their mean \texttt{nRMSE} (averaged across the 10 trials for that cell).
    \item \textbf{Averaging Ranks}: We then calculated the final "Avg. Rank" for each method by averaging these per-cell ranks across two distinct regimes:
    \begin{itemize}
        \item \textbf{Low-Noise Regime}: Averaged across noise levels below 1\% (\texttt{1e-8}, \texttt{1e-6}, \texttt{1e-4}, and \texttt{1e-3}).
        \item \textbf{High-Noise Regime}: Averaged across noise levels of 1\% and above (\texttt{0.01} and \texttt{0.02}).
    \end{itemize}
\end{enumerate}
This methodology ensures that the final rank is a robust measure of a method's performance across a wide variety of conditions, and it prevents a single outlier or a particularly favorable test case from dominating the results.

\subsection{Quantitative Performance Metrics}
To add further quantitative rigor to our analysis, we defined two specific metrics to characterize method performance beyond the simple average error.
\begin{itemize}
    \item \textbf{Noise Robustness:} We define a method's robustness for a given derivative order as the highest noise level at which its \texttt{nRMSE} remains at or below 0.1. A higher value on this metric indicates a method can maintain accuracy under more significant noise conditions. This provides a more concrete measure of robustness than average error alone.
    \item \textbf{High-Order Stability:} To quantify how gracefully a method's performance degrades as the task becomes more difficult, we measure its high-order stability. This is defined as the slope of the log of the \texttt{nRMSE} versus the derivative order, calculated in the low-noise regime. A lower, flatter slope indicates that the method is more stable and its error grows more slowly as it is tasked with computing higher-order derivatives.
\end{itemize}
These metrics provide a complementary view to the main rankings and are used to inform the specific recommendations and trade-offs discussed in our conclusion.

\subsection{Analysis of Coverage Bias}
A critical aspect of a fair benchmark is understanding "coverage bias." Many methods are not designed to compute high-order derivatives. For example, \texttt{Central-FD} and \texttt{TVRegDiff-Julia} in our study only support up to order 1. If we were to rank all methods naively based on their average error across all the tests they passed, these methods would appear artificially superior, as they would only be evaluated on "easy" low-order configurations and would be exempted from the challenging high-order tests where most methods struggle.

To create a fair comparison, we therefore restricted our main summary table to the cohort of "contender" methods that successfully produced results for all tested configurations up to order 5. Methods with partial coverage are not inherently worse, but they should be considered specialists. A practitioner who only needs a first-order derivative might find that \texttt{TVRegDiff-Julia} is an excellent choice, but it cannot be fairly compared in a general-purpose ranking against a method that successfully computes 7th-order derivatives.

\subsection{Performance Degradation by Derivative Order}
A clear pattern emerging from the data is the systematic degradation of performance as the derivative order increases. This is an expected consequence of the ill-posed nature of differentiation. We can characterize this trend in several phases:
\begin{itemize}
    \item \textbf{Orders 0-2 (Low-Order):} In this regime, most contender methods perform well, and the performance differences between them are relatively modest, particularly in low-noise scenarios. The task of smoothing or finding a first or second derivative is not challenging enough to create significant separation between the top methods.
    \item \textbf{Orders 3-5 (Mid-Order):} This is the regime where a clear separation emerges. The task becomes significantly more challenging, and methods without sophisticated noise handling begin to struggle. The performance of GPR and the stronger spectral methods remains high, while simpler spline- and filter-based methods see a substantial drop in accuracy.
    \item \textbf{Orders 6-7 (High-Order):} This regime represents an extreme challenge. Only a very small subset of methods, primarily GPR, are able to produce a usable estimate, and even their errors are significant. For most other methods, the error in this regime constitutes a catastrophic failure. Derivative order is clearly the dominant factor in the difficulty of the estimation problem.
\end{itemize}

Figure~\ref{fig:small_multiples} provides a comprehensive visual illustration of this systematic degradation. The figure shows performance across all eight derivative orders for the top seven methods, clearly demonstrating how error increases with order and how different methods respond to increasing noise levels at each order.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{small_multiples_grid.png}
\caption{\textbf{Performance Across All Derivative Orders.} This 4×2 grid shows nRMSE vs noise level for the top 7 methods at each derivative order (0--7). Each panel illustrates the systematic degradation of performance as order increases. Note how GP-Julia-AD (top performer) maintains relatively stable performance across all orders, while other methods show dramatic degradation beyond order 3. The shaded regions indicate performance quality: green (excellent, nRMSE < 0.1), yellow (good, 0.1--0.3), and orange (acceptable, 0.3--1.0).}
\label{fig:small_multiples}
\end{figure}

\subsection{The Critical Role of Taylor-Mode AD for High-Order Derivatives}
A key technical factor in the performance of high-order derivative estimation is the underlying mechanism of the Automatic Differentiation library used. Naively composing first-order AD operations (i.e., nested forward- or reverse-mode AD) to compute a high-order derivative results in an algorithm with exponential complexity, rendering it infeasible for orders beyond a handful.

The success of the \texttt{GP-Julia-AD} method, for instance, is critically dependent on its use of Taylor-mode AD. This mode is specifically designed to compute high-order derivatives efficiently by propagating a full Taylor series expansion through the computation, rather than just a single derivative value. This approach reduces the computational complexity significantly (often to polynomial time), making the computation of 5th, 6th, and 7th order derivatives tractable. This highlights that for practitioners seeking high-order derivatives, the choice of AD implementation is as important as the choice of the approximant itself.

\subsection{Computational Efficiency: Practical Considerations}
\label{sec:efficiency}

\textbf{Important Caveats:} The computational timing measurements in this study should be interpreted with caution. Unlike the error metrics, which represent fundamental algorithmic characteristics, the timing data has several limitations that prevent it from being elevated to a primary finding:

\begin{itemize}
    \item \textbf{Mixed-language implementations:} Our benchmark includes methods implemented in both Julia and Python, with different optimization levels and library maturity.
    \item \textbf{Small dataset size:} All timing measurements were performed on trajectories with $N=101$ points. Scaling behavior for larger datasets ($N > 1000$) may differ significantly, particularly for methods with non-linear complexity.
    \item \textbf{Implementation quality variation:} Methods vary in optimization sophistication. Some are production-grade libraries, while others are research prototypes.
    \item \textbf{No warm-up or statistical rigor:} Timing measurements represent single runs without JIT warm-up for Julia methods or statistical validation across multiple hardware configurations.
\end{itemize}

Given these limitations, we present the following \textbf{qualitative guidance} based on our observations, rather than quantitative claims:

\textbf{Three-Tier Framework for Method Selection:}

\begin{enumerate}
    \item \textbf{High-Speed Tier (sub-millisecond):} For real-time or large-scale applications where speed is paramount, filtering methods like \texttt{Savitzky-Golay} provide rapid computation with reasonable accuracy in moderate noise conditions. Basic spectral methods (simple FFT-based approaches) also fall in this category. These methods are particularly suitable when computational budget is the primary constraint.

    \item \textbf{Balanced Tier (milliseconds to tens of milliseconds):} For most scientific applications requiring a strong balance between accuracy and speed, spectral methods represent an excellent middle ground. Methods like \texttt{Fourier-GCV}, \texttt{Fourier-Interp}, and high-degree splines (\texttt{Dierckx-5}) provide accuracy approaching that of GPR methods while maintaining practical computational efficiency. This tier is likely optimal for interactive data analysis and medium-scale studies.

    \item \textbf{High-Accuracy Tier (hundreds of milliseconds):} When the highest possible accuracy is the primary concern and computational cost is secondary, Gaussian Process methods provide the lowest estimation errors across all noise levels and derivative orders. Our results show \texttt{GP-Julia-AD} consistently outperforms all other methods in accuracy. The computational investment is justified in applications where estimation quality is critical and the dataset size is moderate.
\end{enumerate}

\textbf{Scaling Considerations:} For very large datasets ($N > 1000$), the cubic scaling of standard GPR implementations may become prohibitive, making fast spectral or filtering methods the only viable options unless sparse or approximate GP methods are employed.

% PARETO PLOT CODE (COMMENTED OUT FOR FUTURE USE)
% Uncomment when timing data has been collected with:
% - Consistent warm-up procedures
% - Multiple hardware configurations
% - Statistical validation (multiple runs)
% - Larger dataset sizes (N > 1000)
%
% \begin{figure}[htbp]
% \centering
% \includegraphics[width=0.9\textwidth]{pareto_frontier.png}
% \caption{\textbf{Speed vs. Accuracy Trade-off: Pareto Frontier.} This plot shows the relationship between mean computation time (x-axis, log scale) and mean nRMSE (y-axis, log scale) for all methods with full coverage of derivative orders 0--5. Diamond markers indicate Pareto-optimal methods—points where no other method is both faster and more accurate. The black dashed line connects the Pareto front. Methods are colored by category (Gaussian Process, Spectral, Spline, etc.). The plot reveals three distinct tiers: (1) sub-millisecond filtering methods like \texttt{Savitzky-Golay}, (2) millisecond-scale balanced methods like \texttt{Dierckx-5} and spectral approaches, and (3) high-accuracy GP methods like \texttt{GP-Julia-AD} that achieve the lowest error at the cost of longer computation time. \textbf{Caveat:} Timing measurements represent implementations of varying optimization levels on a small dataset ($N=101$) and should be interpreted as qualitative guidance rather than definitive benchmarks.}
% \label{fig:pareto}
% \end{figure}

% Removed PyNumDiff analysis subsection - methods are included in main results
