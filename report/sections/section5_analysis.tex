\section{Results and Analysis}
\label{sec:results}

After a comprehensive evaluation across a range of methods, three distinct dynamical systems, and a sweep of noise levels, clear patterns emerge. Performance was analyzed in two regimes: a "low-noise" regime ($\le 0.1\%$ noise) where precision is paramount, and a "high-noise" regime (1-2\% noise) where robustness is key.

The table below summarizes the performance of contender methods that demonstrated full data coverage for derivative orders 0 through 5. Methods are sorted by their overall average rank, giving equal weight to performance in both low- and high-noise regimes. Averages are calculated over orders 0-5.
% should we exclude 0 in the task?

% AUTO-GENERATED by gemini-analysis/generate_exploratory_tables.py
% To regenerate: ./scripts/04_generate_tables.sh
% Alternative versions (orders 3, 7) available as tab_summary_order{3,7}.tex
\input{../build/tables/publication/tab_summary_order5.tex}

\textbf{Our principal findings are as follows:}

\begin{enumerate}
    \item \textbf{Gaussian Process Regression (GPR) is the most robust and accurate method overall.} The Julia GPR implementation (\texttt{GP-Julia-AD}) and the improved Python variants (\texttt{GP-RBF-*}) are the clear winners, consistently occupying the top ranks in both low and high-noise regimes.
    \item \textbf{The optimal method depends on the noise level and derivative order.} While GPR is the best all-arounder, splines like \texttt{Dierckx-5} offer excellent precision in low-noise environments, making them a top choice for cleaner data, particularly at modest derivative orders. In the high-noise regime, the filtering-based \texttt{Savitzky-Golay} provides a computationally cheap and highly effective alternative, ranking solidly in the top half of contenders.
    \item \textbf{Theoretical limits matter.} Many common low-degree spline methods are, by definition, incapable of representing high-order derivatives, limiting their applicability.
    \item \textbf{Dedicated packages offer convenient and robust options, but need a differentiation backend.}   Purpose built libraries are convenient, but our strongest results come from extending them with either analytic or auto-differentiated derivatives of smoothed data.  For this to work best, the library should ideally produce very smooth models, and either expose some of the model internals or be amenable to AD.

\end{enumerate}

The quantitative results in the summary table are powerfully illustrated by the following key visualizations.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{high_noise_fit_comparison.png}
\caption{\textbf{High-Noise Performance (2\% noise, 4th derivative).} Three-panel comparison showing: (top) noisy input data as black points against gray ground truth, emphasizing the challenging signal-to-noise environment; (middle) derivative estimates from four methods representing different algorithmic approaches; (bottom) estimation errors revealing where each method succeeds or fails. The figure shows \texttt{GP-Julia-AD}, \texttt{Savitzky-Golay-Fixed}, \texttt{Dierckx-5}, and \texttt{Fourier-GCV}. The error panel reveals that all methods struggle at the trajectory boundaries.}
\label{fig:high_noise_comp}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{low_noise_fit_comparison.png}
\caption{\textbf{Low-Noise Precision (1e-6 noise, 5th derivative).} Three-panel comparison in a low-noise regime: (top) noisy data points (black) are nearly imperceptible against gray ground truth; (middle) 5th-order derivative estimates from the top three methods; (bottom) estimation errors showing clear performance hierarchy. \texttt{GP-Julia-AD} dominates with RMS error of 272k, while \texttt{Dierckx-5} (3.1M) and \texttt{Fourier-GCV} (5.8M) show substantially larger errors. Even in this clean environment, high-order differentiation remains extremely challenging, with \texttt{GP-Julia-AD} achieving more than 10× better accuracy than competing methods.}
\label{fig:low_noise_comp}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{top_methods_heatmap.png}
\caption{\textbf{Performance Degradation with Increasing Derivative Order.} This heatmap shows the mean nRMSE for top methods at a 1\% noise level. The vertical axis is sorted by average performance across all orders. This visualization clearly shows that \texttt{GP-Julia-AD} maintains low error across all derivative orders, while other methods like \texttt{Dierckx-5} and \texttt{GSS} are highly accurate for low orders but degrade significantly as the order increases.}
\label{fig:heatmap}
\end{figure}

The subsequent subsections detail the methodology and analysis that support these conclusions, starting with an explanation of our ranking approach.

\subsection{Ranking Methodology}
To produce the final summary table, we followed a two-step ranking process:
\begin{enumerate}
    \item \textbf{Per-Cell Ranking}: For each individual experimental cell—defined by a unique combination of \texttt{(ODE\_system, noise\_level, derivative\_order)}—we ranked the contender methods against each other based on their mean \texttt{nRMSE} (averaged across the 10 trials for that cell).
    \item \textbf{Averaging Ranks}: We then calculated the final "Avg. Rank" for each method by averaging these per-cell ranks across two distinct regimes:
    \begin{itemize}
        \item \textbf{Low-Noise Regime}: Averaged across noise levels below 1\% (\texttt{1e-8}, \texttt{1e-6}, \texttt{1e-4}, and \texttt{1e-3}).
        \item \textbf{High-Noise Regime}: Averaged across noise levels of 1\% and above (\texttt{0.01} and \texttt{0.02}).
    \end{itemize}
\end{enumerate}
This methodology ensures that the final rank is a robust measure of a method's performance across a wide variety of conditions, and it prevents a single outlier or a particularly favorable test case from dominating the results.

\subsection{Quantitative Performance Metrics}
To add further quantitative rigor to our analysis, we defined two specific metrics to characterize method performance beyond the simple average error.
\begin{itemize}
    \item \textbf{Noise Robustness:} We define a method's robustness for a given derivative order as the highest noise level at which its \texttt{nRMSE} remains at or below 0.1. A higher value on this metric indicates a method can maintain accuracy under more significant noise conditions. This provides a more concrete measure of robustness than average error alone.
    \item \textbf{High-Order Stability:} To quantify how gracefully a method's performance degrades as the task becomes more difficult, we measure its high-order stability. This is defined as the slope of the log of the \texttt{nRMSE} versus the derivative order, calculated in the low-noise regime. A lower, flatter slope indicates that the method is more stable and its error grows more slowly as it is tasked with computing higher-order derivatives.
\end{itemize}
These metrics provide a complementary view to the main rankings and are used to inform the specific recommendations and trade-offs discussed in our conclusion.

\subsection{Analysis of Coverage Bias}
A critical aspect of a fair benchmark is understanding "coverage bias." Many methods are not designed to compute high-order derivatives. For example, \texttt{Central-FD} and \texttt{TVRegDiff-Julia} in our study only support up to order 1. If we were to rank all methods naively based on their average error across all the tests they passed, these methods would appear artificially superior, as they would only be evaluated on "easy" low-order configurations and would be exempted from the challenging high-order tests where most methods struggle.

To create a fair comparison, we therefore restricted our main summary table to the cohort of "contender" methods that successfully produced results for all tested configurations up to order 5. Methods with partial coverage are not inherently worse, but they should be considered specialists. A practitioner who only needs a first-order derivative might find that \texttt{TVRegDiff-Julia} is an excellent choice, but it cannot be fairly compared in a general-purpose ranking against a method that successfully computes 7th-order derivatives.

\subsection{Performance Degradation by Derivative Order}
A clear pattern emerging from the data is the systematic degradation of performance as the derivative order increases. This is an expected consequence of the ill-posed nature of differentiation. We can characterize this trend in several phases:
\begin{itemize}
    \item \textbf{Orders 0-2 (Low-Order):} In this regime, most contender methods perform well, and the performance differences between them are relatively modest, particularly in low-noise scenarios. The task of smoothing or finding a first or second derivative is not challenging enough to create significant separation between the top methods.
    \item \textbf{Orders 3-5 (Mid-Order):} This is the regime where a clear separation emerges. The task becomes significantly more challenging, and methods without sophisticated noise handling begin to struggle. The performance of GPR and the stronger spectral methods remains high, while simpler spline- and filter-based methods see a substantial drop in accuracy.
    \item \textbf{Orders 6-7 (High-Order):} This regime represents an extreme challenge. Only a very small subset of methods, primarily GPR, are able to produce a usable estimate, and even their errors are significant. For most other methods, the error in this regime constitutes a catastrophic failure. Derivative order is clearly the dominant factor in the difficulty of the estimation problem.
\end{itemize}

Figure~\ref{fig:small_multiples} provides a comprehensive visual illustration of this systematic degradation. The figure shows performance across all eight derivative orders for the top seven methods, clearly demonstrating how error increases with order and how different methods respond to increasing noise levels at each order.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{small_multiples_grid.png}
\caption{\textbf{Performance Across All Derivative Orders.} This 4×2 grid shows nRMSE vs noise level for the top 7 methods at each derivative order (0--7). Each panel illustrates the systematic degradation of performance as order increases. Note how GP-Julia-AD (top performer) maintains relatively stable performance across all orders, while other methods show dramatic degradation beyond order 3. The shaded regions indicate performance quality: green (excellent, nRMSE < 0.1), yellow (good, 0.1--0.3), and orange (acceptable, 0.3--1.0).}
\label{fig:small_multiples}
\end{figure}

\subsection{The Critical Role of Taylor-Mode AD for High-Order Derivatives}
A key technical factor in the performance of high-order derivative estimation is the underlying mechanism of the Automatic Differentiation library used. Naively composing first-order AD operations (i.e., nested forward- or reverse-mode AD) to compute a high-order derivative results in an algorithm with exponential complexity, rendering it infeasible for orders beyond a handful.

The success of the \texttt{GP-Julia-AD} method, for instance, is critically dependent on its use of Taylor-mode AD. This mode is specifically designed to compute high-order derivatives efficiently by propagating a full Taylor series expansion through the computation, rather than just a single derivative value. This approach reduces the computational complexity significantly (often to polynomial time), making the computation of 5th, 6th, and 7th order derivatives tractable. This highlights that for practitioners seeking high-order derivatives, the choice of AD implementation is as important as the choice of the approximant itself.

% Computational efficiency considerations moved to Practitioner's Guide in Conclusion
% Removed PyNumDiff analysis subsection - methods are included in main results
