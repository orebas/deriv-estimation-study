\section{Conclusion}
\label{sec:conclusion}

This comprehensive study evaluated a wide array of numerical methods for the estimation of high-order derivatives from noisy data. After a detailed investigation that included a multi-stage filtering of methods and a deep dive into implementation details, our findings are clear and decisive.

\subsection{Summary of Key Findings}

\begin{itemize}
    \item \textbf{Gaussian Process Regression (GPR) is the most robust and accurate method overall.} GPR methods consistently occupy the top ranks in both low- and high-noise regimes, making them the most reliable choice for general-purpose derivative estimation.
    \item \textbf{The optimal method depends on the use case.} While GPR is the best all-arounder, splines like \texttt{Spline-Dierckx-5} offer excellent precision for low-noise data, while spectral methods provide moderate accuracy at interactive speeds. For real-time applications requiring sub-10ms response, \texttt{Savitzky-Golay} remains the most practical choice despite lower accuracy.
    \item \textbf{Derivative order is the dominant difficulty factor.} Performance degrades systematically with increasing order across all methods. The problem becomes significantly more challenging beyond order 3, and only a handful of methods produce usable results at orders 6 or 7.
    \item \textbf{Implementation quality is a critical method characteristic.} Our study found significant performance differences between different software packages implementing the same underlying algorithm, highlighting that practitioners must consider the quality of a specific implementation, not just the theoretical method.
\end{itemize}

\textbf{The primary recommendation of this work is that for practitioners who require accurate high-order derivatives from real-world, noisy signals, Gaussian Process Regression is the most reliable and effective starting point.}

\subsection{Computational Cost and Method Selection}
\label{sec:practitioners_guide}

Computational costs vary significantly across methods, from milliseconds for filters to seconds for Gaussian Processes. The smoothing/fitting step dominates timing; derivative order has minimal impact on computational cost. Table~\ref{tab:timing} shows representative methods spanning the speed-accuracy spectrum.

\begin{table}[!ht]
\centering
\caption{Computational Cost vs Accuracy Trade-Off}
\label{tab:timing}
\input{../build/tables/publication/tab_timing_comparison.tex}
\end{table}

\textbf{Speed-Accuracy Tradeoff:}

Figure~\ref{fig:pareto} illustrates the speed-accuracy tradeoff at noise level 0.0001 and derivative order 3. Remarkably, only four methods define the Pareto frontier:
\begin{itemize}
    \item \texttt{SavitzkyGolay-Fixed} (1.1 ms): Fastest method on the frontier (nRMSE = 0.52)
    \item \texttt{ButterworthSpline\_Python} (4.0 ms): Fast filtering approach (nRMSE = 0.38)
    \item \texttt{Spline-Dierckx-5} (8.8 ms): High-degree spline interpolation (nRMSE = 0.32)
    \item \texttt{GP-TaylorAD-Julia} (1.78 s): Best accuracy, three orders of magnitude slower (nRMSE = 0.14)
\end{itemize}

The large gap between sub-millisecond filters and second-scale GP methods suggests an opportunity for methods that better balance speed and accuracy in the 10--100 ms range.

\begin{figure}[!ht]
\centering
\includegraphics[width=0.8\textwidth]{pareto_frontier.png}
\caption{\textbf{Speed-accuracy tradeoff for derivative estimation methods.} Log-log plot showing normalized RMSE versus computation time for all methods at noise level 0.0001 and derivative order 3. Methods are colored by category. The dashed line connects the four methods on the Pareto frontier, showing the tradeoff between fast filters and accurate GP methods.}
\label{fig:pareto}
\end{figure}

\FloatBarrier
\textbf{Practical Recommendations:}

When accuracy is paramount, \texttt{GP-TaylorAD-Julia} consistently delivers the best results across all derivative orders. For applications with timing constraints:

\begin{itemize}
    \item \textbf{Real-time (< 10 ms):} \texttt{SavitzkyGolay-Fixed}, \texttt{ButterworthSpline\_Python}, or \texttt{Spline-Dierckx-5} provide acceptable accuracy for orders 0--3.

    \item \textbf{Interactive (10--100 ms):} \texttt{Fourier-GCV} offers good accuracy/speed balance.

    \item \textbf{Offline analysis:} \texttt{GP-TaylorAD-Julia} (1.8 s) remains practical for batch processing.
\end{itemize}

Note that GP methods scale as $O(N^3)$ with data size, while spectral methods scale as $O(N \log N)$, making the latter preferable for very large datasets.

\subsection{Future Work}

This benchmark, while comprehensive, is not exhaustive. Several avenues for future research are immediately apparent:

\begin{enumerate}
    \item \textbf{Testing on Diverse Signals:} Our study used ODEs that produce smooth, analytic signals. Future work should include testing on more challenging signals, such as those with discontinuities, sharp peaks, or chaotic behavior.
    \item \textbf{Evaluating Alternative Noise Models:} The real world is not always Gaussian. A valuable extension would be to evaluate method performance under different noise models, such as multiplicative, Poisson, or heavy-tailed noise.
    \item \textbf{Larger-Scale Problems:} Our study was limited to a modest number of data points. Investigating how method performance, particularly computational cost, scales to much larger datasets ($N > 1000$) would be of great practical interest.
\end{enumerate}

\subsection{Limitations and Scope}

Our benchmark is restricted to scalar time series on uniform grids with additive Gaussian noise, generated from a small set of ODE systems. We do not consider irregular sampling, multivariate outputs, non-Gaussian noise models, or signals with discontinuities. These remain important directions for future investigation. Additionally, our focus on high-order derivatives (up to order 7) reflects the needs of dynamical systems identification; applications requiring only first or second derivatives may find different trade-offs optimal.

\subsection{Broader Implications: The Case for a Composable, Differentiable Ecosystem}

Our findings also underscore a broader trend in scientific computing: the immense value of composable and differentiable software packages. The "Approximant-AD" framework is only possible when libraries for data modeling (e.g., Gaussian Processes) can seamlessly pass their results to libraries for automatic differentiation.

While not all numerical packages are readily differentiable out-of-the-box, our experience suggests that many can be adapted with modest effort. We encourage researchers and developers to prioritize differentiability in their own software and to contribute upstream to make foundational libraries in the ecosystem compatible with AD frameworks. Such efforts create a virtuous cycle, unlocking powerful new hybrid methodologies that benefit the entire scientific community, far beyond the immediate application of derivative estimation.
