\section{Methods Evaluated}
\label{sec:methods}

This section describes the 33 derivative estimation methods analyzed in this study: 24 baseline methods plus 9 adaptive hyperparameter selection variants (Section~\ref{sec:adaptive_methods}). Three additional baseline methods were evaluated but excluded from final analysis due to implementation failures documented in Section~\ref{sec:exclusions}.

\subsection{Summary}
\label{sec:methods_summary}

Table~\ref{tab:methods_summary} lists all 33 methods with key characteristics. The 9 adaptive methods are detailed in Section~\ref{sec:adaptive_methods}.

\begin{table}[htbp]
\centering
\caption{Methods Evaluated (33 methods: 24 baseline + 9 adaptive; 3 baseline candidates excluded per Section~\ref{sec:exclusions})}
\label{tab:methods_summary}
\small
\begin{tabular}{llllcc}
\toprule
Method & Category & Language & Key Parameter(s) & Complexity & Coverage \\
\midrule
GP-Julia-AD & Gaussian Process & Julia & Length scale (MLE) & $O(n^3)$ & 56/56 \\
GP\_RBF\_Python & Gaussian Process & Python & Length scale (MLE) & $O(n^3)$ & 56/56 \\
GP\_RBF\_Iso\_Python & Gaussian Process & Python & Length scale (MLE) & $O(n^3)$ & 56/56 \\
gp\_rbf\_mean & Gaussian Process & Python & Length scale (MLE) & $O(n^3)$ & 56/56 \\
AAA-HighPrec & Rational & Julia & Tolerance=$10^{-13}$ & $O(n^2)$ & 56/56 \\
AAA-LowPrec & Rational & Julia & Tolerance=$10^{-13}$ & $O(n^2)$ & 56/56 \\
Fourier-Interp & Spectral & Julia & Filter frac=0.4 & $O(n \log n)$ & 56/56 \\
Dierckx-5 & Spline & Julia & Smoothing (GCV) & $O(n)$ & 42/56 \\
\multicolumn{6}{l}{\small (Table continues with remaining 16 methods; see full version in supplementary materials)} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Coverage notes:}
\begin{itemize}
    \item Full coverage (56/56): Tested across all 8 derivative orders $\times$ 7 noise levels
    \item Partial coverage: Missing high orders (typically 6--7) or restricted to orders 0--1 due to library/implementation limitations
\end{itemize}

\subsection{Gaussian Process Methods}
\label{sec:gp_methods}

Gaussian Process (GP) regression provides a principled Bayesian framework for function approximation and derivative estimation. GPs place a prior over functions and compute posterior distributions conditioned on observed data. Derivatives are obtained by differentiating the GP posterior mean.

\subsubsection{GP-Julia-AD}

\textbf{Mathematical Formulation:}

A Gaussian Process defines a distribution over functions $f \sim \text{GP}(m(x), k(x,x'))$ where $m$ is the mean function (typically 0) and $k$ is the covariance kernel. Given observations $y = f(x) + \varepsilon$ with noise $\varepsilon \sim \mathcal{N}(0, \sigma^2_n)$, the posterior predictive distribution is:

\begin{align}
f(x^*) | y &\sim \mathcal{N}(\mu^*(x^*), (\sigma^*)^2(x^*)) \\
\mu^*(x^*) &= k_*(K + \sigma^2_n I)^{-1} y \\
(\sigma^*)^2(x^*) &= k_{**} - k_*^T(K + \sigma^2_n I)^{-1} k_*
\end{align}

where $K_{ij} = k(x_i, x_j)$, $k_* = [k(x^*, x_1), \ldots, k(x^*, x_n)]^T$, and $k_{**} = k(x^*, x^*)$.

\textbf{Derivative estimation:} The $n$-th derivative of the posterior mean is obtained by differentiating the kernel function:

\begin{equation}
\frac{d^n \mu^*(x^*)}{dx^{*n}} = \left[\frac{d^n k(x^*, x_1)}{dx^{*n}}, \ldots, \frac{d^n k(x^*, x_n)}{dx^{*n}}\right] (K + \sigma^2_n I)^{-1} y
\end{equation}

\textbf{Kernel:} Squared Exponential (SE) / RBF kernel:
\begin{equation}
k(x, x') = \sigma^2_f \exp\left(-\frac{(x - x')^2}{2\ell^2}\right)
\end{equation}
where $\sigma^2_f$ is signal variance and $\ell$ is length scale controlling smoothness.

\textbf{Hyperparameter optimization:} Length scale $\ell$, signal variance $\sigma^2_f$, and noise variance $\sigma^2_n$ optimized via Maximum Likelihood Estimation (MLE) using L-BFGS-B with 3 random restarts (seeded deterministically per trial; Section~\ref{sec:hyperparameters}).

\textbf{Implementation:} GaussianProcesses.jl with ForwardDiff.jl for automatic differentiation of kernel derivatives up to order 7.

\textbf{Computational note:} For high-order derivatives ($n \geq 5$), ForwardDiff uses nested dual numbers, increasing per-point evaluation cost. \TODO{Verify if input/output normalization and jitter term $(K + \sigma^2I + \varepsilon_{\text{jitter}} \cdot I)$ were used for numerical stability}

\textbf{Built-in uncertainty:} Predictive variance $(\sigma^*)^2(x)$ quantifies confidence in derivative estimates (not evaluated in this benchmark).

\textbf{Computational complexity:} $O(n^3)$ for training (Cholesky factorization of $K + \sigma^2I$), $O(n)$ per prediction point (vector-matrix products)

\textbf{Coverage:} Full (56/56 configurations)

\subsubsection{Other GP Variants}

\textbf{GP\_RBF\_Python, GP\_RBF\_Iso\_Python, gp\_rbf\_mean:} Implementations using scikit-learn. \TODO{Clarify derivative computation method --- scikit-learn GPR does not natively provide derivative predictions. Specify if finite-difference approximation of predictive mean was used (step size, scheme) or if kernel derivatives were manually implemented}

Coverage: Full (56/56 configurations)

\subsection{Rational Approximation Methods}
\label{sec:rational_methods}

Rational approximation represents functions as ratios of polynomials: $r(x) = p(x)/q(x)$. Unlike polynomial interpolation, rational functions can capture singularities and exhibit better convergence for smooth functions.

\subsubsection{AAA-HighPrec (Adaptive Antoulas-Anderson Algorithm)}

\textbf{Mathematical Formulation:}

The AAA algorithm constructs a rational interpolant in barycentric form:

\begin{equation}
r(z) = \frac{\sum_i w_i f_i / (z - z_i)}{\sum_i w_i / (z - z_i)}
\end{equation}

where $\{z_i, f_i\}$ are support points (subset of data) and $\{w_i\}$ are weights.

\textbf{Algorithm (simplified):}
\begin{enumerate}
    \item Initialize support set with point having maximum residual
    \item Iteration $k$:
    \begin{itemize}
        \item Solve least-squares problem (typically via SVD of Loewner matrix) for weights $w_i$ minimizing $\|r(z_j) - f_j\|$ over non-support points
        \item Add point with maximum residual to support set
    \end{itemize}
    \item Terminate when max residual $< $ tolerance ($10^{-13}$)
    \item Differentiate analytically: $dr/dz$ computed via quotient rule on barycentric form
\end{enumerate}

\textbf{Key implementation detail:} Uses BigFloat (256-bit) arithmetic throughout.

\textbf{Strengths:}
\begin{itemize}
    \item Deterministic (no stochastic optimization)
    \item Adaptive complexity (automatically selects number of support points $m$)
\end{itemize}

\textbf{Potential failure modes:}
\begin{itemize}
    \item Spurious poles can appear near evaluation points
    \item High-order rational function derivatives may accumulate error
\end{itemize}

\textbf{Computational complexity:} $O(n m^2)$ where $m$ is number of support points (typically $m \ll n$); reported as $O(n^2)$ heuristically

\textbf{Coverage:} Full (56/56 configurations)

\subsection{Spectral Methods}
\label{sec:spectral_methods}

Spectral methods represent functions in terms of global basis functions (Fourier, Chebyshev, or trigonometric polynomials) and compute derivatives by differentiating the basis functions.

\subsubsection{Fourier-Interp (FFT-Based Spectral Differentiation)}

\textbf{Mathematical Formulation:}

Represent signal as Fourier series on domain $[a,b]$ with $N$ points:
\begin{equation}
f(x) \approx \sum_{k=-N/2}^{N/2} c_k \exp(i k \omega x)
\end{equation}
where $\omega = 2\pi / (b-a)$ is the fundamental frequency.

Derivatives via differentiation in frequency domain:
\begin{equation}
\frac{d^n f}{dx^n} = \sum (i k \omega)^n c_k \exp(i k \omega x)
\end{equation}

\textbf{Algorithm:}
\begin{enumerate}
    \item Symmetrically extend signal to enforce periodicity (note: Lotka-Volterra trajectories are oscillatory but not strictly periodic; Section~\ref{sec:controls})
    \item Compute FFT to obtain Fourier coefficients $\{c_k\}$
    \item Multiply by $(i k \omega)^n$ for $n$-th derivative
    \item Apply low-pass filter: retain lower 40\% of frequency spectrum (filter fraction = 0.4; pre-tuned per Section~\ref{sec:hyperparameters})
    \item Inverse FFT to obtain derivative in spatial domain
\end{enumerate}

\textbf{Filtering details:} \TODO{Specify passband definition (fraction of Nyquist or absolute $k_{\max}$), taper/roll-off, FFT normalization convention, and extension strategy (even/odd/mirror)}

\textbf{Implementation:} FFTW.jl for fast Fourier transforms

\textbf{Strengths:}
\begin{itemize}
    \item Extremely fast: $O(n \log n)$ via FFT
    \item Suitable for smooth, periodic or near-periodic signals
\end{itemize}

\textbf{Weaknesses:}
\begin{itemize}
    \item Periodicity assumption may introduce edge artifacts for non-periodic signals
    \item Fixed pre-tuned filter fraction (Section~\ref{sec:hyperparameters} documents potential advantage over per-dataset tuning)
\end{itemize}

\textbf{Computational complexity:} $O(n \log n)$

\textbf{Coverage:} Full (56/56 configurations)

\subsection{Finite Difference Methods}
\label{sec:fd_methods}

Finite difference methods approximate derivatives using linear combinations of function values on a stencil.

\subsubsection{Central-FD (Central Finite Differences)}

\textbf{Mathematical formulation:}

For evenly-spaced grid with spacing $h$, central difference stencils approximate derivatives:

\textbf{First derivative (3-point stencil):}
\begin{equation}
f'(x) \approx \frac{f(x+h) - f(x-h)}{2h}
\end{equation}
Truncation error: $O(h^2)$

\textbf{Second derivative (3-point stencil):}
\begin{equation}
f''(x) \approx \frac{f(x+h) - 2f(x) + f(x-h)}{h^2}
\end{equation}

\textbf{Higher orders:} Require wider stencils; $n$-th derivative requires $\geq (n+1)$ points

\textbf{Implementation:} Julia implementation using standard central difference stencils

\textbf{Critical limitation:} Library/implementation provides stencils only up to 1st order. Coverage restricted to orders 0--1.

\textbf{Strengths:}
\begin{itemize}
    \item Simple, well-understood
    \item Fast: $O(n)$
    \item No hyperparameters
\end{itemize}

\textbf{Weaknesses:}
\begin{itemize}
    \item Noise amplification: For additive noise with standard deviation $\sigma$, the 3-point central difference amplifies noise to $O(\sigma/h)$ in the derivative estimate
    \item Boundary treatment requires asymmetric stencils or extrapolation
\end{itemize}

\textbf{Coverage:} Partial (14/56 configurations, orders 0--1 only)

\subsection{Spline Methods}
\label{sec:spline_methods}

Spline methods fit piecewise polynomials with continuity constraints at knots, then differentiate the spline.

\subsubsection{Dierckx-5 (Smoothing Spline with GCV)}

\textbf{Mathematical formulation:}

Minimizes penalized least squares:
\begin{equation}
\sum_i (y_i - s(x_i))^2 + \lambda \int (s''(x))^2 dx
\end{equation}
where $s(x)$ is a spline of degree $k=5$ and $\lambda$ is the smoothing parameter controlling bias-variance tradeoff.

\textbf{Smoothing parameter selection:} Generalized Cross-Validation (GCV) minimizes predicted mean squared error on held-out data (Section~\ref{sec:hyperparameters})

\textbf{Implementation:} Dierckx.jl (wrapper around FORTRAN FITPACK library)

\textbf{Derivative support:} Degree-5 splines support derivatives up to order 5 (degree-$k$ splines provide well-defined derivatives up to order $k$).

\textbf{Strengths:}
\begin{itemize}
    \item Automatic smoothing via GCV
    \item Fast: $O(n)$ with banded linear systems
    \item Natural boundary conditions ($d^2s/dx^2 = 0$ at endpoints)
\end{itemize}

\textbf{Weaknesses:}
\begin{itemize}
    \item Limited to orders 0--5 (degree-5 spline maximum for this implementation)
\end{itemize}

\textbf{Coverage:} Partial (42/56 configurations, orders 0--5 only)

\subsection{Local Polynomial Methods}
\label{sec:local_poly}

\subsubsection{Savitzky-Golay (Local Polynomial Regression)}

\textbf{Mathematical formulation:}

For each point $x_i$, fit a polynomial of degree $d$ to points within a sliding window of width $w$ via least squares:
\begin{equation}
p(x) = \sum_{j=0}^d a_j (x - x_i)^j
\end{equation}

The derivative is the polynomial derivative evaluated at $x_i$: $f^{(n)}(x_i) \approx n! \, a_n$

\textbf{Closed form:} Can be expressed as discrete convolution with fixed filter coefficients (depends on window width $w$, polynomial degree $d$, and derivative order $n$)

\textbf{Implementation:} Julia implementation

\textbf{Parameter mapping:} \TODO{Specify window size $w$ and polynomial degree $d$ as functions of derivative order $n$ and boundary handling strategy}

\textbf{Strengths:}
\begin{itemize}
    \item Fast: $O(n)$ via convolution
    \item No global optimization
\end{itemize}

\textbf{Weaknesses:}
\begin{itemize}
    \item Fixed window size can be suboptimal
    \item Boundary handling via window shrinking
\end{itemize}

\textbf{Coverage:} Full (56/56 configurations)

\subsection{Regularization Methods}
\label{sec:regularization}

\subsubsection{TVRegDiff-Julia (Total Variation Regularized Differentiation)}

\textbf{Mathematical formulation:}

Estimates derivative $u = df/dx$ by minimizing an objective combining data fidelity and total variation regularization.

\textbf{Objective:} \TODO{Define precise objective function --- specify operator $E$ linking derivative to observations, data fidelity term structure (e.g., cumulative sum/integration operator $A$), and boundary conditions. Standard TVRegDiff minimizes $(1/2)\|A u - f\|^2 + \alpha \text{TV}(u)$; clarify if this variant is used or specify the exact formulation}

\textbf{Total variation:} $\text{TV}(u) = \sum_i |u_{i+1} - u_i|$ promotes piecewise constant derivatives

\textbf{Algorithm:} Iterative optimization (ADMM or similar) with automatic tuning of regularization parameter $\alpha$

\textbf{Implementation:} Julia implementation with convergence tolerance $10^{-6}$, max 100 iterations

\textbf{Strengths:}
\begin{itemize}
    \item Preserves discontinuities (edges)
    \item Robust to outliers
\end{itemize}

\textbf{Weaknesses:}
\begin{itemize}
    \item Limited to orders 0--1 in current implementation
    \item Computationally expensive: $O(n)$ per iteration $\times$ up to 100 iterations
\end{itemize}

\textbf{Coverage:} Partial (14/56 configurations, orders 0--1 only)

\subsubsection{Trend Filtering (TrendFilter-k2, TrendFilter-k7)}

\textbf{Formulation:} Trend filtering with penalty order $k$ (penalizes $k$-th discrete derivative)

\textbf{Objective:} Minimizes $(1/2)\|y - x\|^2 + \lambda \|D^k x\|_1$ where $D^k$ is $k$-th order discrete difference matrix

\textbf{Implementation:} Convex optimization via \TODO{Specify solver (specialized trend filtering algorithm like PDAS, path algorithm, or generic QP/ADMM) and complexity implications}

\textbf{Coverage:} Full (56/56 configurations)

\subsection{Adaptive Hyperparameter Selection Methods}
\label{sec:adaptive_methods}

A critical challenge in derivative estimation is selecting appropriate hyperparameters (polynomial degree, number of harmonics, smoothing tolerance) that balance bias and variance. Traditional methods use fixed, pre-tuned parameters across all noise levels and derivative orders. This section describes 9 new adaptive methods that automatically select hyperparameters based on the data characteristics, particularly noise level estimation.

\subsubsection{Noise Estimation Techniques}

Two noise estimation methods are used across the adaptive methods:

\textbf{Wavelet-based MAD (Median Absolute Deviation):}
\begin{equation}
\hat{\sigma} = \frac{\text{MAD}(\text{HF}_{wavelet})}{0.6745}
\end{equation}
where $\text{HF}_{wavelet}$ denotes level-1 detail coefficients from Daubechies-4 wavelet decomposition with symmetric boundary extension. The constant 0.6745 is the MAD-to-standard-deviation conversion factor for Gaussian noise (Donoho-Johnstone, 1994). This method is robust to signal content as it isolates noise in the high-frequency wavelet decomposition.

\textbf{Second-order difference method:}
\begin{equation}
\hat{\sigma} = \sqrt{\frac{1}{6(n-2)} \sum_{i=2}^{n-1} (y_{i+1} - 2y_i + y_{i-1})^2}
\end{equation}
This exploits the fact that $\text{Var}(y_{i+1} - 2y_i + y_{i-1}) = 6\sigma^2$ for additive Gaussian noise with variance $\sigma^2$, while smooth signal components are suppressed by the second-order difference operator.

\subsubsection{Adaptive Polynomial Methods}

\textbf{Chebyshev-AICc (Adaptive Chebyshev via AICc):}

Uses small-sample corrected Akaike Information Criterion \cite{hurvich1989regression} (AICc) to select polynomial degree:
\begin{equation}
\text{AICc}(d) = n \log(\text{RSS}_d/n) + 2(d+1) + \frac{2(d+1)(d+2)}{n-d-2}
\end{equation}
where $\text{RSS}_d$ is residual sum of squares for degree-$d$ polynomial, and $n$ is sample size. The correction term $\frac{2(d+1)(d+2)}{n-d-2}$ is critical for small samples to prevent overfitting.

\textbf{Algorithm:}
\begin{enumerate}
    \item Scale input domain to $[-1, 1]$ (standard Chebyshev domain)
    \item Fit Chebyshev polynomials for degrees $d \in \{3, 4, \ldots, \min(15, n/3)\}$
    \item Compute AICc for each degree
    \item Select $d^* = \argmin_d \text{AICc}(d)$
    \item Differentiate the degree-$d^*$ Chebyshev polynomial analytically
\end{enumerate}

\textbf{Implementation:} Python using numpy.polynomial.Chebyshev

\textbf{Coverage:} Full (56/56 configurations)

\subsubsection{Adaptive Spectral Methods}

\textbf{Fourier-GCV (Fourier with GCV Harmonics Selection):}

Uses Generalized Cross-Validation to select number of Fourier harmonics $M$:
\begin{equation}
\text{GCV}(M) = \frac{n \cdot \text{RSS}_M}{(n - \text{df}_M)^2}
\end{equation}
where $\text{df}_M = 2M + 1$ is effective degrees of freedom for $M$ harmonics.

\textbf{Algorithm:}
\begin{enumerate}
    \item Compute FFT of signal
    \item For $M \in \{3, 5, 7, \ldots, \lfloor n/4 \rfloor\}$:
    \begin{itemize}
        \item Retain lowest $M$ frequencies, zero out rest
        \item Compute inverse FFT (reconstruction)
        \item Calculate GCV score
    \end{itemize}
    \item Select $M^* = \argmin_M \text{GCV}(M)$
    \item Differentiate in frequency domain: multiply by $(ik\omega)^n$, inverse FFT
\end{enumerate}

\textbf{Coverage:} Full (56/56 configurations)

\textbf{Fourier-FFT-Adaptive (Noise-Adaptive Spectral Filtering):}

Combines wavelet MAD noise estimation with adaptive frequency cutoff:
\begin{enumerate}
    \item Estimate noise $\hat{\sigma}$ via wavelet MAD
    \item Compute signal-to-noise ratio: $\text{SNR} = \text{std}(y) / \hat{\sigma}$
    \item Set adaptive cutoff: $k_{max} = \lfloor n/2 \cdot \min(0.5, \max(0.2, 0.1 \cdot \log_{10}(\text{SNR}))) \rfloor$
    \item Retain frequencies $|k| \leq k_{max}$, differentiate via $(ik)^n$ multiplication
\end{enumerate}

\textbf{Rationale:} High SNR allows more frequencies (less aggressive smoothing); low SNR requires aggressive filtering to prevent noise amplification.

\textbf{Coverage:} Full (56/56 configurations)

\textbf{Fourier-Continuation-Adaptive:}

Dual adaptive strategy combining Fourier continuation (to reduce endpoint artifacts) with GCV-based harmonics selection. Uses reflected boundary extension for better periodicity, then applies GCV as in Fourier-GCV.

\textbf{Coverage:} Full (56/56 configurations)

\textbf{ad\_trig\_adaptive (Adaptive Trigonometric via GCV):}

Similar to Fourier-GCV but uses trigonometric basis functions. Applies GCV to select optimal number of trigonometric terms for least-squares fitting.

\textbf{Coverage:} Full (56/56 configurations)

\subsubsection{Adaptive Rational Approximation Methods}

The AAA (Adaptive Antoulas-Anderson) algorithm inherently adapts the number of support points via iterative greedy selection, but the termination tolerance $\text{tol}$ critically affects performance. The following methods adaptively set the AAA tolerance based on estimated noise level:

\textbf{AAA-Python-Adaptive-Wavelet:}

Uses wavelet MAD noise estimation to set AAA tolerance:
\begin{equation}
\text{tol} = \max(10^{-13}, C \cdot \hat{\sigma})
\end{equation}
where $C = 10$ is an empirically tuned safety factor. Implements AAA via baryrat library (Python port of Chebfun's AAA).

\textbf{Derivative computation:} Uses baryrat's built-in derivative method (analytic differentiation of barycentric form). Limited to derivative orders 0--5 in standard implementation.

\textbf{Coverage:} Partial (42/56 configurations, orders 0--5 only)

\textbf{AAA-Python-Adaptive-Diff2:}

Identical to AAA-Python-Adaptive-Wavelet except uses second-order difference noise estimation instead of wavelet MAD. Provides comparison of noise estimation methods' impact on AAA performance.

\textbf{Coverage:} Partial (42/56 configurations, orders 0--5 only)

\textbf{AAA-JAX-Adaptive-Wavelet \& AAA-JAX-Adaptive-Diff2:}

\textbf{Key innovation:} Uses JAX (Google's automatic differentiation library) to compute arbitrary-order derivatives of the AAA rational approximant, enabling orders 6--7 which are unavailable in standard baryrat.

\textbf{Derivative computation via nested forward-mode automatic differentiation:}
\begin{equation}
\frac{d^n r}{dx^n}\bigg|_{x=x_0} = \text{jax.jvp}^{(n)}(r)(x_0)
\end{equation}
where $\text{jax.jvp}^{(n)}$ denotes $n$ nested applications of JAX's Jacobian-vector product (forward-mode AD).

\textbf{Algorithm:}
\begin{enumerate}
    \item Estimate noise $\hat{\sigma}$ (wavelet MAD or diff2)
    \item Compute AAA approximation $r(x)$ using baryrat with tolerance $10 \cdot \hat{\sigma}$
    \item Re-implement barycentric evaluation formula using \texttt{jax.numpy} to create JAX-traceable function $r(x)$
    \item Apply $n$ nested \texttt{jax.jvp()} calls to obtain $n$-th derivative
\end{enumerate}

\textbf{Computational complexity:} Nested forward-mode AD has exponential cost scaling with derivative order $n$. High-order derivatives (6--7) can take 8--20 minutes per configuration due to repeated compilations and nested differentiation overhead.

\textbf{Coverage:} Full (56/56 configurations) --- first AAA-based method achieving orders 6--7

\textbf{Performance note:} Section~\ref{sec:results} documents that AAA-JAX methods enable order 7 derivatives but do not solve the fundamental AAA instability issue at high orders (see Section~\ref{sec:aaa_failure}). The automatic differentiation is numerically stable, but the underlying rational approximant still exhibits catastrophic error growth.

\subsection{Implementation Notes}
\label{sec:implementation_notes}

\textbf{Cross-language consistency:} Where methods exist in both Julia and Python, parameters were matched to ensure fair comparison (Section~\ref{sec:controls}). Large performance discrepancies despite parameter parity led to exclusion of the inferior implementation (see Section~\ref{sec:exclusions} for detailed documentation and justification).

\textbf{Reproducibility:} All methods use fixed random seeds for any stochastic components (GP hyperparameter initialization, SVR grid search randomization). Julia methods benefit from just-in-time compilation with 1 warm-up run excluded from timing (Section~\ref{sec:controls}).

\textbf{Partial coverage rationale:}
\begin{itemize}
    \item \textbf{Orders 6--7 missing for many methods:} Polynomial degree or library limitations (splines require degree $\geq$ order; filters/regularization limited by implementation)
    \item \textbf{Orders 0--1 only for some:} Implementation design restrictions (TVRegDiff, Central-FD library constraints)
\end{itemize}

\textbf{Parameter tuning fairness:} All tunable methods received equivalent optimization effort (Section~\ref{sec:hyperparameters}). GPs and splines use per-dataset optimization (MLE/GCV); Fourier methods use pre-tuned fixed parameters, which may confer an advantage (Section~\ref{sec:hyperparameters} discusses this methodological concern).

\textbf{Methodological transparency:} Several method descriptions contain TODO markers indicating implementation details that should be verified from code/documentation before final publication. These do not affect the validity of the experimental results (which depend only on the actual implementations run), but are noted for complete methodological reproducibility.
