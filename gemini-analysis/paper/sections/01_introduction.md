# 1. Introduction

Derivative estimation from noisy data is a fundamental and notoriously ill-posed problem spanning computational science, engineering, and data analysis. Small noise perturbations in the input signal can produce arbitrarily large errors in derivative estimates, a challenge that intensifies exponentially with the derivative order. Despite its difficulty, the problem is of critical importance in numerous fields:

*   **Dynamical Systems Identification:** Inferring governing equations from time-series data requires accurate derivative estimates to identify differential equations.
*   **Signal Processing:** Edge detection, feature extraction, and change-point analysis all depend on robust derivative computation.
*   **Control Systems:** Model-predictive and adaptive control rely on real-time derivative estimation for system state feedback.
*   **Physics-Informed Machine Learning:** Enforcing physical conservation laws (PDE constraints) requires differentiating neural network outputs with respect to noisy inputs.
*   **Scientific Data Analysis:** Estimating rates of change from experimental measurements, such as reaction rates from concentration data or acceleration from position measurements.

Our investigation is motivated by challenges encountered in the field of parameter estimation for Ordinary Differential Equations (ODEs). Techniques in this domain often require evaluating expressions that contain high-order derivatives of the system's state variables, which are themselves derived from noisy time-series data. The recurring difficulty in finding a reliable method for this prerequisite step prompted the benchmark study presented herein.

This background also suggests that ODEs provide a uniquely suitable testbed for such a study. The structure of an ODE system, $\dot{\mathbf{x}} = f(\mathbf{x})$, provides a natural and computable source of high-precision ground-truth data. Higher-order derivatives can be generated by repeatedly differentiating the system's equations, e.g., $\ddot{\mathbf{x}} = \frac{\partial f}{\partial \mathbf{x}} \dot{\mathbf{x}}$. This process, performed symbolically or via system augmentation, allows for the creation of a validation dataset without resorting to a separate numerical differentiation method, thus avoiding a circular argument. The methods benchmarked are therefore tested against the very class of systems for which they are often intended.

Our own prior work demonstrated that for noiseless data, the combination of rational function approximation (specifically, the AAA algorithm) followed by automatic differentiation is a highly effective method for this task. The present study was born from the challenge of extending this success to real-world, noisy data, a necessary step for applying these techniques to practical problems in science and engineering.

### The Need for a Comprehensive Benchmark

Despite the widespread importance of this problem, systematic comparative evaluation of derivative estimation methods is lacking in the literature. Practitioners seeking to choose a method are faced with a confusing landscape of options, and prior studies often suffer from several limitations:

1.  **Limited Scope:** Benchmarks typically evaluate a small handful of methods on low-order derivatives (0-2), neglecting the high-order derivatives where method performance often diverges dramatically.
2.  **Single-Noise Regime:** Many studies test either noiseless or high-noise cases, missing the crucial performance transitions that occur across different noise levels.
3.  **Incomplete Coverage Transparency:** Methods that fail at high orders or under high noise are often simply excluded from results without clear documentation, potentially biasing the conclusions.

This study was designed to address these gaps by providing a comprehensive, transparent, and reproducible benchmark across a wide range of methods, derivative orders, and noise regimes, with the goal of providing clear, actionable guidance for practitioners.

## 1.3 An Overview of the Present Study

This paper presents the findings of a comprehensive benchmark of derivative estimation methods. We adopt a narrative structure to document our investigation, which began with broad hypotheses and concluded with a specific, data-driven recommendation. By systematically eliminating certain classes of methods—some on the basis of empirical failure, others due to theoretical limitations—we narrowed the field to a small group of effective contenders.

The performance of these final methods was evaluated across three distinct dynamical systems and a wide spectrum of noise levels, leading to a clear and practical conclusion. Our aim is to provide the reader with not just a ranking, but an understanding of *why* certain methods succeed where others fail, and a clear guide for selecting the appropriate tool for their own work.
